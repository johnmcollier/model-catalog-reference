<!--
Sourced from: https://huggingface.co/microsoft/Phi-3.5-mini-instruct
-->
# **Microsoft Phi-3.5-mini-instruct**

## **Model Description**

Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.

### **Model Architecture**
Phi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini.

### **Developed By**
Microsoft

### **Model Repository**
[huggingface.co/meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)

### **Release Date**
August 2024

### **Licensing**
The model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/LICENSE).

## **Intended Use**
<!--
Sourced from: https://huggingface.co/microsoft/Phi-3.5-mini-instruct#intended-uses
-->

### **Primary Use Cases**

The model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:

1. Memory/compute constrained environments
2. Latency bound scenarios
3. Strong reasoning (especially code, math and logic)